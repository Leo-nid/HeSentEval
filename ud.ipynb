{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RuSentEval.probing.arguments import ProbingArguments\n",
    "from RuSentEval.probing.experiment import Experiment\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start probing for the following tasks: ['coordination_inversion']\n",
      "Probing the coordination_inversion task...\n",
      "/home/leon/hse/HeSentEval/data/he/coordination_inversion.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b260454a3a42a28bfba1f08252f95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.693\n",
      "Validation Loss: 0.696\n",
      "Validation Score: 0.491\n",
      "Test Loss: 0.691\n",
      "Test Score: 0.506\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.693\n",
      "Validation Loss: 0.696\n",
      "Validation Score: 0.488\n",
      "Test Loss: 0.69\n",
      "Test Score: 0.555\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.692\n",
      "Validation Loss: 0.696\n",
      "Validation Score: 0.506\n",
      "Test Loss: 0.696\n",
      "Test Score: 0.5\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.691\n",
      "Validation Loss: 0.693\n",
      "Validation Score: 0.52\n",
      "Test Loss: 0.693\n",
      "Test Score: 0.503\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.646\n",
      "Validation Loss: 0.658\n",
      "Validation Score: 0.673\n",
      "Test Loss: 0.64\n",
      "Test Score: 0.666\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.574\n",
      "Validation Loss: 0.6\n",
      "Validation Score: 0.754\n",
      "Test Loss: 0.536\n",
      "Test Score: 0.765\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.555\n",
      "Validation Loss: 0.589\n",
      "Validation Score: 0.754\n",
      "Test Loss: 0.505\n",
      "Test Score: 0.779\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.573\n",
      "Validation Loss: 0.596\n",
      "Validation Score: 0.743\n",
      "Test Loss: 0.51\n",
      "Test Score: 0.817\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.597\n",
      "Validation Loss: 0.619\n",
      "Validation Score: 0.713\n",
      "Test Loss: 0.55\n",
      "Test Score: 0.747\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.646\n",
      "Validation Loss: 0.651\n",
      "Validation Score: 0.687\n",
      "Test Loss: 0.614\n",
      "Test Score: 0.718\n",
      "******************************\n",
      "\n",
      "Start probing for the following tasks: ['coordination_inversion']\n",
      "Probing the coordination_inversion task...\n",
      "/home/leon/hse/HeSentEval/data/he/coordination_inversion.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858ac9d0447344fc99df39678c4b94cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.694\n",
      "Validation Loss: 0.696\n",
      "Validation Score: 0.518\n",
      "Test Loss: 0.691\n",
      "Test Score: 0.512\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.691\n",
      "Validation Loss: 0.697\n",
      "Validation Score: 0.52\n",
      "Test Loss: 0.693\n",
      "Test Score: 0.5\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.686\n",
      "Validation Loss: 0.693\n",
      "Validation Score: 0.538\n",
      "Test Loss: 0.683\n",
      "Test Score: 0.538\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.598\n",
      "Validation Loss: 0.64\n",
      "Validation Score: 0.684\n",
      "Test Loss: 0.588\n",
      "Test Score: 0.674\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.642\n",
      "Validation Loss: 0.674\n",
      "Validation Score: 0.632\n",
      "Test Loss: 0.634\n",
      "Test Score: 0.613\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.687\n",
      "Validation Loss: 0.695\n",
      "Validation Score: 0.535\n",
      "Test Loss: 0.689\n",
      "Test Score: 0.552\n",
      "******************************\n",
      "\n",
      "Start probing for the following tasks: ['coordination_inversion']\n",
      "Probing the coordination_inversion task...\n",
      "/home/leon/hse/HeSentEval/data/he/coordination_inversion.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/xlm-v-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57182f09f88748cf8282f6a17ce7aa41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.692\n",
      "Validation Loss: 0.698\n",
      "Validation Score: 0.5\n",
      "Test Loss: 0.698\n",
      "Test Score: 0.5\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.692\n",
      "Validation Loss: 0.7\n",
      "Validation Score: 0.503\n",
      "Test Loss: 0.695\n",
      "Test Score: 0.52\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.69\n",
      "Validation Loss: 0.698\n",
      "Validation Score: 0.509\n",
      "Test Loss: 0.689\n",
      "Test Score: 0.52\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.687\n",
      "Validation Loss: 0.692\n",
      "Validation Score: 0.529\n",
      "Test Loss: 0.683\n",
      "Test Score: 0.552\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.0001\n",
      "Train Loss: 0.573\n",
      "Validation Loss: 0.615\n",
      "Validation Score: 0.716\n",
      "Test Loss: 0.546\n",
      "Test Score: 0.738\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.517\n",
      "Validation Loss: 0.563\n",
      "Validation Score: 0.772\n",
      "Test Loss: 0.487\n",
      "Test Score: 0.785\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.503\n",
      "Validation Loss: 0.539\n",
      "Validation Score: 0.804\n",
      "Test Loss: 0.46\n",
      "Test Score: 0.805\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.489\n",
      "Validation Loss: 0.531\n",
      "Validation Score: 0.801\n",
      "Test Loss: 0.434\n",
      "Test Score: 0.837\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.52\n",
      "Validation Loss: 0.556\n",
      "Validation Score: 0.801\n",
      "Test Loss: 0.48\n",
      "Test Score: 0.817\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.518\n",
      "Validation Loss: 0.559\n",
      "Validation Score: 0.775\n",
      "Test Loss: 0.48\n",
      "Test Score: 0.788\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.575\n",
      "Validation Loss: 0.601\n",
      "Validation Score: 0.766\n",
      "Test Loss: 0.536\n",
      "Test Score: 0.776\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.694\n",
      "Validation Loss: 0.693\n",
      "Validation Score: 0.538\n",
      "Test Loss: 0.693\n",
      "Test Score: 0.5\n",
      "******************************\n",
      "\n",
      "Start probing for the following tasks: ['coordination_inversion']\n",
      "Probing the coordination_inversion task...\n",
      "/home/leon/hse/HeSentEval/data/he/coordination_inversion.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at avichr/heBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b07119e7cb447cf81bbcf795abdc125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.686\n",
      "Validation Loss: 0.702\n",
      "Validation Score: 0.515\n",
      "Test Loss: 0.689\n",
      "Test Score: 0.52\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.684\n",
      "Validation Loss: 0.7\n",
      "Validation Score: 0.503\n",
      "Test Loss: 0.687\n",
      "Test Score: 0.529\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.678\n",
      "Validation Loss: 0.686\n",
      "Validation Score: 0.567\n",
      "Test Loss: 0.67\n",
      "Test Score: 0.584\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.0001\n",
      "Train Loss: 0.539\n",
      "Validation Loss: 0.572\n",
      "Validation Score: 0.757\n",
      "Test Loss: 0.507\n",
      "Test Score: 0.765\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.49\n",
      "Validation Loss: 0.524\n",
      "Validation Score: 0.795\n",
      "Test Loss: 0.447\n",
      "Test Score: 0.797\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.473\n",
      "Validation Loss: 0.518\n",
      "Validation Score: 0.816\n",
      "Test Loss: 0.439\n",
      "Test Score: 0.794\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.47\n",
      "Validation Loss: 0.525\n",
      "Validation Score: 0.798\n",
      "Test Loss: 0.439\n",
      "Test Score: 0.802\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.484\n",
      "Validation Loss: 0.541\n",
      "Validation Score: 0.789\n",
      "Test Loss: 0.461\n",
      "Test Score: 0.811\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.495\n",
      "Validation Loss: 0.556\n",
      "Validation Score: 0.766\n",
      "Test Loss: 0.475\n",
      "Test Score: 0.794\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.508\n",
      "Validation Loss: 0.571\n",
      "Validation Score: 0.763\n",
      "Test Loss: 0.491\n",
      "Test Score: 0.776\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.0001\n",
      "Train Loss: 0.524\n",
      "Validation Loss: 0.586\n",
      "Validation Score: 0.749\n",
      "Test Loss: 0.505\n",
      "Test Score: 0.776\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.524\n",
      "Validation Loss: 0.595\n",
      "Validation Score: 0.725\n",
      "Test Loss: 0.509\n",
      "Test Score: 0.75\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "#          \"sent_len\", \n",
    "#          \"tokens_cnt\", \n",
    "#          \"past_present\", \n",
    "#          \"bigram_shift\",\n",
    "#          \"subj_number\",\n",
    "         \"coordination_inversion\",\n",
    "        ]\n",
    "models = [\"xlm-roberta-base\", \"distilbert-base-multilingual-cased\", \"facebook/xlm-v-base\", \"avichr/heBERT\"]\n",
    "\n",
    "args = ProbingArguments()\n",
    "args.clf = \"logreg\"\n",
    "args.data_dir = \"data/he\"\n",
    "result_dir = \"results/he\"\n",
    "\n",
    "for model in models:\n",
    "  experiment = Experiment(tasks, model, args, result_dir)\n",
    "  experiment.run()\n",
    "  del experiment\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start probing for the following tasks: ['sentence_length', 'past_present', 'bigram_shift', 'subj_number', 'coordination_inversion']\n",
      "Probing the sentence_length task...\n",
      "/home/leon/hse/HeSentEval/SentEval/data/probing/sentence_length.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfea0920fffe44239399d61ae1b8d126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.273\n",
      "Validation Loss: 0.248\n",
      "Validation Score: 0.957\n",
      "Test Loss: 0.166\n",
      "Test Score: 0.955\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.257\n",
      "Validation Loss: 0.234\n",
      "Validation Score: 0.958\n",
      "Test Loss: 0.154\n",
      "Test Score: 0.959\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.272\n",
      "Validation Loss: 0.248\n",
      "Validation Score: 0.953\n",
      "Test Loss: 0.162\n",
      "Test Score: 0.952\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.307\n",
      "Validation Loss: 0.278\n",
      "Validation Score: 0.942\n",
      "Test Loss: 0.187\n",
      "Test Score: 0.941\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "         \"sentence_length\", \n",
    "         \"past_present\", \n",
    "         \"bigram_shift\",\n",
    "         \"subj_number\",\n",
    "         \"coordination_inversion\",\n",
    "        ]\n",
    "models = [\"xlm-roberta-base\", \"distilbert-base-multilingual-cased\", \"facebook/xlm-v-base\"]\n",
    "\n",
    "args = ProbingArguments()\n",
    "args.clf = \"logreg\"\n",
    "args.data_dir = \"SentEval/data/probing\"\n",
    "result_dir = \"results/en_full\"\n",
    "\n",
    "for model in models:\n",
    "  experiment = Experiment(tasks, model, args, result_dir)\n",
    "  experiment.run()\n",
    "  del experiment\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start probing for the following tasks: ['coordination_inversion']\n",
      "Probing the coordination_inversion task...\n",
      "/home/leon/hse/HeSentEval/data/he/coordination_inversion.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mbart-large-50 were not used when initializing MBartModel: ['final_logits_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing MBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b503bcb15b71415889dadc10a872fb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.701\n",
      "Validation Loss: 0.701\n",
      "Validation Score: 0.509\n",
      "Test Loss: 0.697\n",
      "Test Score: 0.512\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.705\n",
      "Validation Loss: 0.706\n",
      "Validation Score: 0.512\n",
      "Test Loss: 0.702\n",
      "Test Score: 0.509\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.705\n",
      "Validation Loss: 0.703\n",
      "Validation Score: 0.52\n",
      "Test Loss: 0.701\n",
      "Test Score: 0.509\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.705\n",
      "Validation Loss: 0.696\n",
      "Validation Score: 0.523\n",
      "Test Loss: 0.695\n",
      "Test Score: 0.52\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.707\n",
      "Validation Loss: 0.696\n",
      "Validation Score: 0.526\n",
      "Test Loss: 0.693\n",
      "Test Score: 0.526\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.711\n",
      "Validation Loss: 0.702\n",
      "Validation Score: 0.512\n",
      "Test Loss: 0.697\n",
      "Test Score: 0.512\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.649\n",
      "Validation Loss: 0.665\n",
      "Validation Score: 0.652\n",
      "Test Loss: 0.646\n",
      "Test Score: 0.631\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.61\n",
      "Validation Loss: 0.635\n",
      "Validation Score: 0.716\n",
      "Test Loss: 0.6\n",
      "Test Score: 0.695\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.544\n",
      "Validation Loss: 0.597\n",
      "Validation Score: 0.754\n",
      "Test Loss: 0.523\n",
      "Test Score: 0.744\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.511\n",
      "Validation Loss: 0.562\n",
      "Validation Score: 0.76\n",
      "Test Loss: 0.478\n",
      "Test Score: 0.773\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.557\n",
      "Validation Loss: 0.574\n",
      "Validation Score: 0.775\n",
      "Test Loss: 0.511\n",
      "Test Score: 0.773\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.467\n",
      "Validation Loss: 0.51\n",
      "Validation Score: 0.807\n",
      "Test Loss: 0.381\n",
      "Test Score: 0.869\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "#          \"sent_len\", \n",
    "#          \"tokens_cnt\", \n",
    "#          \"past_present\", \n",
    "#          \"bigram_shift\",\n",
    "#          \"subj_number\",\n",
    "         \"coordination_inversion\",\n",
    "        ]\n",
    "# name of the HuggingFace model; you can adjust the code for your model\n",
    "model = \"facebook/mbart-large-50\"\n",
    "args = ProbingArguments()\n",
    "\n",
    "# args.clf == \"logreg\" or args.clf == \"mlp\" for linear/non-linear classification\n",
    "args.clf = \"logreg\"\n",
    "args.device = \"cpu\"\n",
    "args.data_dir = \"data/he\"\n",
    "result_dir = \"results/he\"\n",
    "\n",
    "experiment = Experiment(tasks, model, args, result_dir)\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "         \"sentence_length\", \n",
    "         \"past_present\", \n",
    "         \"bigram_shift\",\n",
    "         \"subj_number\",\n",
    "         \"coordination_inversion\",\n",
    "        ]\n",
    "# name of the HuggingFace model; you can adjust the code for your model\n",
    "model = \"facebook/mbart-large-50\"\n",
    "args = ProbingArguments()\n",
    "\n",
    "# args.clf == \"logreg\" or args.clf == \"mlp\" for linear/non-linear classification\n",
    "args.clf = \"logreg\"\n",
    "args.device = \"cpu\"\n",
    "args.data_dir = \"SentEval/data/probing\"\n",
    "result_dir = \"results/en_full\"\n",
    "\n",
    "experiment = Experiment(tasks, model, args, result_dir)\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start probing for the following tasks: ['sent_len', 'tokens_cnt', 'past_present', 'bigram_shift', 'subj_number', 'coordination_inversion']\n",
      "Probing the sent_len task...\n",
      "/home/leon/hse/HeSentEval/data/he/sent_len.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a0170a62e74280844c1a656feb4b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec1cdbcf2b143f4a6728e6b2d6f1507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HeNLP/HeRo were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6f31ccd79e4bbaa2ecc96bc9b81a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a86f337f71848eaba1c5807c07fa344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f88252344be43fb95fccf8aab3f2254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2eff233d0648d2a3176cb63b4a1ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/3.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860fcfd05e4946b5bf94894ae38ac980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100bef9bef394ad2ad73634de29513c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.581\n",
      "Validation Loss: 0.614\n",
      "Validation Score: 0.808\n",
      "Test Loss: 0.482\n",
      "Test Score: 0.776\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.603\n",
      "Validation Loss: 0.629\n",
      "Validation Score: 0.802\n",
      "Test Loss: 0.502\n",
      "Test Score: 0.774\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.6\n",
      "Validation Loss: 0.628\n",
      "Validation Score: 0.796\n",
      "Test Loss: 0.499\n",
      "Test Score: 0.776\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.648\n",
      "Validation Loss: 0.661\n",
      "Validation Score: 0.784\n",
      "Test Loss: 0.533\n",
      "Test Score: 0.75\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.73\n",
      "Validation Loss: 0.726\n",
      "Validation Score: 0.753\n",
      "Test Loss: 0.584\n",
      "Test Score: 0.725\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.756\n",
      "Validation Loss: 0.749\n",
      "Validation Score: 0.728\n",
      "Test Loss: 0.607\n",
      "Test Score: 0.722\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.765\n",
      "Validation Loss: 0.753\n",
      "Validation Score: 0.737\n",
      "Test Loss: 0.608\n",
      "Test Score: 0.72\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.762\n",
      "Validation Loss: 0.75\n",
      "Validation Score: 0.753\n",
      "Test Loss: 0.609\n",
      "Test Score: 0.727\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.779\n",
      "Validation Loss: 0.763\n",
      "Validation Score: 0.734\n",
      "Test Loss: 0.623\n",
      "Test Score: 0.717\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.816\n",
      "Validation Loss: 0.798\n",
      "Validation Score: 0.731\n",
      "Test Loss: 0.656\n",
      "Test Score: 0.699\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.887\n",
      "Validation Loss: 0.867\n",
      "Validation Score: 0.706\n",
      "Test Loss: 0.707\n",
      "Test Score: 0.697\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.841\n",
      "Validation Loss: 0.854\n",
      "Validation Score: 0.706\n",
      "Test Loss: 0.707\n",
      "Test Score: 0.696\n",
      "******************************\n",
      "\n",
      "Probing the tokens_cnt task...\n",
      "/home/leon/hse/HeSentEval/data/he/tokens_cnt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HeNLP/HeRo were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8952bc1e2c864603a84b6047d3f0e3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.0001\n",
      "Train Loss: 0.706\n",
      "Validation Loss: 0.731\n",
      "Validation Score: 0.735\n",
      "Test Loss: 0.61\n",
      "Test Score: 0.736\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.693\n",
      "Validation Loss: 0.725\n",
      "Validation Score: 0.742\n",
      "Test Loss: 0.6\n",
      "Test Score: 0.736\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.732\n",
      "Validation Loss: 0.761\n",
      "Validation Score: 0.716\n",
      "Test Loss: 0.62\n",
      "Test Score: 0.737\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.739\n",
      "Validation Loss: 0.769\n",
      "Validation Score: 0.706\n",
      "Test Loss: 0.622\n",
      "Test Score: 0.734\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.809\n",
      "Validation Loss: 0.82\n",
      "Validation Score: 0.691\n",
      "Test Loss: 0.658\n",
      "Test Score: 0.713\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.833\n",
      "Validation Loss: 0.842\n",
      "Validation Score: 0.681\n",
      "Test Loss: 0.671\n",
      "Test Score: 0.704\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.84\n",
      "Validation Loss: 0.847\n",
      "Validation Score: 0.68\n",
      "Test Loss: 0.675\n",
      "Test Score: 0.704\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.0001\n",
      "Train Loss: 0.838\n",
      "Validation Loss: 0.85\n",
      "Validation Score: 0.684\n",
      "Test Loss: 0.677\n",
      "Test Score: 0.699\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.857\n",
      "Validation Loss: 0.871\n",
      "Validation Score: 0.684\n",
      "Test Loss: 0.696\n",
      "Test Score: 0.692\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.894\n",
      "Validation Loss: 0.907\n",
      "Validation Score: 0.676\n",
      "Test Loss: 0.723\n",
      "Test Score: 0.69\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.889\n",
      "Validation Loss: 0.917\n",
      "Validation Score: 0.679\n",
      "Test Loss: 0.719\n",
      "Test Score: 0.706\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.969\n",
      "Validation Loss: 1.004\n",
      "Validation Score: 0.642\n",
      "Test Loss: 0.798\n",
      "Test Score: 0.687\n",
      "******************************\n",
      "\n",
      "Probing the past_present task...\n",
      "/home/leon/hse/HeSentEval/data/he/past_present.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HeNLP/HeRo were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc01dd5f171499e862a3fe9a80a105e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.437\n",
      "Validation Loss: 0.436\n",
      "Validation Score: 0.826\n",
      "Test Loss: 0.396\n",
      "Test Score: 0.827\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.433\n",
      "Validation Loss: 0.428\n",
      "Validation Score: 0.835\n",
      "Test Loss: 0.388\n",
      "Test Score: 0.833\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.422\n",
      "Validation Loss: 0.424\n",
      "Validation Score: 0.836\n",
      "Test Loss: 0.383\n",
      "Test Score: 0.835\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.415\n",
      "Validation Loss: 0.42\n",
      "Validation Score: 0.846\n",
      "Test Loss: 0.374\n",
      "Test Score: 0.831\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.391\n",
      "Validation Loss: 0.398\n",
      "Validation Score: 0.858\n",
      "Test Loss: 0.362\n",
      "Test Score: 0.844\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.398\n",
      "Validation Loss: 0.398\n",
      "Validation Score: 0.862\n",
      "Test Loss: 0.361\n",
      "Test Score: 0.842\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.393\n",
      "Validation Loss: 0.395\n",
      "Validation Score: 0.862\n",
      "Test Loss: 0.358\n",
      "Test Score: 0.84\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.392\n",
      "Validation Loss: 0.395\n",
      "Validation Score: 0.854\n",
      "Test Loss: 0.357\n",
      "Test Score: 0.847\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.385\n",
      "Validation Loss: 0.386\n",
      "Validation Score: 0.857\n",
      "Test Loss: 0.356\n",
      "Test Score: 0.858\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.38\n",
      "Validation Loss: 0.384\n",
      "Validation Score: 0.859\n",
      "Test Loss: 0.347\n",
      "Test Score: 0.859\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.381\n",
      "Validation Loss: 0.386\n",
      "Validation Score: 0.859\n",
      "Test Loss: 0.352\n",
      "Test Score: 0.861\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.393\n",
      "Validation Loss: 0.394\n",
      "Validation Score: 0.856\n",
      "Test Loss: 0.361\n",
      "Test Score: 0.847\n",
      "******************************\n",
      "\n",
      "Probing the bigram_shift task...\n",
      "/home/leon/hse/HeSentEval/data/he/bigram_shift.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HeNLP/HeRo were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c5da9a5a27495fb4894f5ebdc3d54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.694\n",
      "Validation Loss: 0.698\n",
      "Validation Score: 0.512\n",
      "Test Loss: 0.704\n",
      "Test Score: 0.495\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.683\n",
      "Validation Loss: 0.69\n",
      "Validation Score: 0.548\n",
      "Test Loss: 0.692\n",
      "Test Score: 0.513\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.666\n",
      "Validation Loss: 0.682\n",
      "Validation Score: 0.589\n",
      "Test Loss: 0.679\n",
      "Test Score: 0.556\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.662\n",
      "Validation Loss: 0.681\n",
      "Validation Score: 0.598\n",
      "Test Loss: 0.68\n",
      "Test Score: 0.559\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.651\n",
      "Validation Loss: 0.675\n",
      "Validation Score: 0.606\n",
      "Test Loss: 0.667\n",
      "Test Score: 0.574\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.622\n",
      "Validation Loss: 0.637\n",
      "Validation Score: 0.663\n",
      "Test Loss: 0.628\n",
      "Test Score: 0.649\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.567\n",
      "Validation Loss: 0.583\n",
      "Validation Score: 0.718\n",
      "Test Loss: 0.549\n",
      "Test Score: 0.71\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.53\n",
      "Validation Loss: 0.554\n",
      "Validation Score: 0.754\n",
      "Test Loss: 0.493\n",
      "Test Score: 0.756\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.0001\n",
      "Train Loss: 0.492\n",
      "Validation Loss: 0.522\n",
      "Validation Score: 0.769\n",
      "Test Loss: 0.452\n",
      "Test Score: 0.774\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.501\n",
      "Validation Loss: 0.522\n",
      "Validation Score: 0.784\n",
      "Test Loss: 0.453\n",
      "Test Score: 0.794\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.512\n",
      "Validation Loss: 0.529\n",
      "Validation Score: 0.787\n",
      "Test Loss: 0.454\n",
      "Test Score: 0.788\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.525\n",
      "Validation Loss: 0.547\n",
      "Validation Score: 0.758\n",
      "Test Loss: 0.484\n",
      "Test Score: 0.765\n",
      "******************************\n",
      "\n",
      "Probing the subj_number task...\n",
      "/home/leon/hse/HeSentEval/data/he/subj_number.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HeNLP/HeRo were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89402f78c9dc4c76aed0246bae5fb2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.458\n",
      "Validation Loss: 0.438\n",
      "Validation Score: 0.84\n",
      "Test Loss: 0.409\n",
      "Test Score: 0.796\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.46\n",
      "Validation Loss: 0.447\n",
      "Validation Score: 0.83\n",
      "Test Loss: 0.427\n",
      "Test Score: 0.791\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.522\n",
      "Validation Loss: 0.512\n",
      "Validation Score: 0.816\n",
      "Test Loss: 0.49\n",
      "Test Score: 0.786\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.507\n",
      "Validation Loss: 0.495\n",
      "Validation Score: 0.816\n",
      "Test Loss: 0.464\n",
      "Test Score: 0.806\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.511\n",
      "Validation Loss: 0.499\n",
      "Validation Score: 0.84\n",
      "Test Loss: 0.477\n",
      "Test Score: 0.811\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.508\n",
      "Validation Loss: 0.493\n",
      "Validation Score: 0.84\n",
      "Test Loss: 0.455\n",
      "Test Score: 0.796\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.514\n",
      "Validation Loss: 0.503\n",
      "Validation Score: 0.835\n",
      "Test Loss: 0.464\n",
      "Test Score: 0.782\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.51\n",
      "Validation Loss: 0.493\n",
      "Validation Score: 0.83\n",
      "Test Loss: 0.452\n",
      "Test Score: 0.777\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.509\n",
      "Validation Loss: 0.492\n",
      "Validation Score: 0.835\n",
      "Test Loss: 0.446\n",
      "Test Score: 0.791\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.508\n",
      "Validation Loss: 0.493\n",
      "Validation Score: 0.84\n",
      "Test Loss: 0.453\n",
      "Test Score: 0.782\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.508\n",
      "Validation Loss: 0.489\n",
      "Validation Score: 0.83\n",
      "Test Loss: 0.441\n",
      "Test Score: 0.791\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.483\n",
      "Validation Loss: 0.465\n",
      "Validation Score: 0.825\n",
      "Test Loss: 0.418\n",
      "Test Score: 0.796\n",
      "******************************\n",
      "\n",
      "Probing the coordination_inversion task...\n",
      "/home/leon/hse/HeSentEval/data/he/coordination_inversion.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HeNLP/HeRo were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366f40a875144150b5fc195cde0119ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features...:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.684\n",
      "Validation Loss: 0.7\n",
      "Validation Score: 0.515\n",
      "Test Loss: 0.689\n",
      "Test Score: 0.529\n",
      "******************************\n",
      "\n",
      "Layer: 2...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.692\n",
      "Validation Loss: 0.706\n",
      "Validation Score: 0.494\n",
      "Test Loss: 0.69\n",
      "Test Score: 0.526\n",
      "******************************\n",
      "\n",
      "Layer: 3...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.691\n",
      "Validation Loss: 0.704\n",
      "Validation Score: 0.518\n",
      "Test Loss: 0.695\n",
      "Test Score: 0.517\n",
      "******************************\n",
      "\n",
      "Layer: 4...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.1\n",
      "Train Loss: 0.686\n",
      "Validation Loss: 0.699\n",
      "Validation Score: 0.529\n",
      "Test Loss: 0.69\n",
      "Test Score: 0.526\n",
      "******************************\n",
      "\n",
      "Layer: 5...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.679\n",
      "Validation Loss: 0.691\n",
      "Validation Score: 0.532\n",
      "Test Loss: 0.672\n",
      "Test Score: 0.558\n",
      "******************************\n",
      "\n",
      "Layer: 6...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.599\n",
      "Validation Loss: 0.635\n",
      "Validation Score: 0.687\n",
      "Test Loss: 0.59\n",
      "Test Score: 0.698\n",
      "******************************\n",
      "\n",
      "Layer: 7...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.574\n",
      "Validation Loss: 0.606\n",
      "Validation Score: 0.731\n",
      "Test Loss: 0.544\n",
      "Test Score: 0.741\n",
      "******************************\n",
      "\n",
      "Layer: 8...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.0001\n",
      "Train Loss: 0.498\n",
      "Validation Loss: 0.545\n",
      "Validation Score: 0.766\n",
      "Test Loss: 0.448\n",
      "Test Score: 0.802\n",
      "******************************\n",
      "\n",
      "Layer: 9...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.522\n",
      "Validation Loss: 0.561\n",
      "Validation Score: 0.757\n",
      "Test Loss: 0.462\n",
      "Test Score: 0.811\n",
      "******************************\n",
      "\n",
      "Layer: 10...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.001\n",
      "Train Loss: 0.494\n",
      "Validation Loss: 0.533\n",
      "Validation Score: 0.789\n",
      "Test Loss: 0.437\n",
      "Test Score: 0.823\n",
      "******************************\n",
      "\n",
      "Layer: 11...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 0.01\n",
      "Train Loss: 0.542\n",
      "Validation Loss: 0.566\n",
      "Validation Score: 0.787\n",
      "Test Loss: 0.498\n",
      "Test Score: 0.802\n",
      "******************************\n",
      "\n",
      "Layer: 12...\n",
      "Training logreg with param L2 1e-05...\n",
      "Training logreg with param L2 0.0001...\n",
      "Training logreg with param L2 0.001...\n",
      "Training logreg with param L2 0.01...\n",
      "Training logreg with param L2 0.1...\n",
      "Best L2: 1e-05\n",
      "Train Loss: 0.573\n",
      "Validation Loss: 0.606\n",
      "Validation Score: 0.734\n",
      "Test Loss: 0.564\n",
      "Test Score: 0.741\n",
      "******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "         \"sent_len\", \n",
    "         \"tokens_cnt\", \n",
    "         \"past_present\", \n",
    "         \"bigram_shift\",\n",
    "         \"subj_number\",\n",
    "         \"coordination_inversion\",\n",
    "        ]\n",
    "models = [\"HeNLP/HeRo\"]\n",
    "\n",
    "args = ProbingArguments()\n",
    "args.clf = \"logreg\"\n",
    "args.data_dir = \"data/he\"\n",
    "result_dir = \"results/he\"\n",
    "\n",
    "for model in models:\n",
    "  experiment = Experiment(tasks, model, args, result_dir)\n",
    "  experiment.run()\n",
    "  del experiment\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07e41a1a4dc009d922bf40092eb1eca2dc280ea2ddba03d42088569804e377fb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
